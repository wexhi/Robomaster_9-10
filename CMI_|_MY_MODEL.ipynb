{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 81933,
          "databundleVersionId": 9643020,
          "sourceType": "competition"
        },
        {
          "sourceId": 9761094,
          "sourceType": "datasetVersion",
          "datasetId": 5977374
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "CMI | MY_MODEL",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wexhi/Robomaster_9-10/blob/main/CMI_%7C_MY_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "PP99Apk6BSlC"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "child_mind_institute_problematic_internet_use_path = kagglehub.competition_download('child-mind-institute-problematic-internet-use')\n",
        "wexhicy_tabnet_path = kagglehub.dataset_download('wexhicy/tabnet')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "9h-OegAfBSlE"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# # For example, here's several helpful packages to load\n",
        "\n",
        "# import numpy as np # linear algebra\n",
        "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# # Input data files are available in the read-only \"../input/\" directory\n",
        "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "# import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-10-30T13:30:35.807542Z",
          "iopub.execute_input": "2024-10-30T13:30:35.808181Z",
          "iopub.status.idle": "2024-10-30T13:30:35.820455Z",
          "shell.execute_reply.started": "2024-10-30T13:30:35.808122Z",
          "shell.execute_reply": "2024-10-30T13:30:35.81897Z"
        },
        "trusted": true,
        "id": "-6SgUpo5BSlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install /kaggle/input/tabnet/pytorch_tabnet-4.1.0-py3-none-any.whl"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-30T13:30:35.823394Z",
          "iopub.execute_input": "2024-10-30T13:30:35.824281Z",
          "iopub.status.idle": "2024-10-30T13:30:50.679322Z",
          "shell.execute_reply.started": "2024-10-30T13:30:35.824215Z",
          "shell.execute_reply": "2024-10-30T13:30:50.677467Z"
        },
        "trusted": true,
        "id": "O7Xy7Tw_BSlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORT"
      ],
      "metadata": {
        "id": "PMC-YlL3BSlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.optimize import minimize\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.optimizers import Adam\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from colorama import Fore, Style\n",
        "from IPython.display import clear_output\n",
        "import warnings\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from pytorch_tabnet.callbacks import Callback\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.options.display.max_columns = None\n",
        "\n",
        "SEED = 42\n",
        "n_splits = 5"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-30T13:30:50.681669Z",
          "iopub.execute_input": "2024-10-30T13:30:50.682274Z",
          "iopub.status.idle": "2024-10-30T13:30:50.706739Z",
          "shell.execute_reply.started": "2024-10-30T13:30:50.682211Z",
          "shell.execute_reply": "2024-10-30T13:30:50.705228Z"
        },
        "trusted": true,
        "id": "M_7cfcGcBSlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function define and read data"
      ],
      "metadata": {
        "id": "kFi7T4-QBSlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file(filename, dirname):\n",
        "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
        "    df.drop('step', axis=1, inplace=True)\n",
        "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
        "\n",
        "def load_time_series(dirname) -> pd.DataFrame:\n",
        "    ids = os.listdir(dirname)\n",
        "\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
        "\n",
        "    stats, indexes = zip(*results)\n",
        "\n",
        "    df = pd.DataFrame(stats, columns=[f\"Stat_{i}\" for i in range(len(stats[0]))])\n",
        "    df['id'] = indexes\n",
        "\n",
        "    return df\n",
        "\n",
        "def feature_engineering(df, is_drope_season = True):\n",
        "    if is_drope_season:\n",
        "        season_cols = [col for col in df.columns if 'Season' in col]\n",
        "        df = df.drop(season_cols, axis=1)\n",
        "\n",
        "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
        "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
        "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
        "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
        "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
        "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
        "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
        "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
        "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
        "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
        "    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
        "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
        "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
        "    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
        "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
        "\n",
        "\n",
        "    return df\n",
        "\n",
        "def quadratic_weighted_kappa(y_true, y_pred):\n",
        "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
        "\n",
        "def threshold_Rounder(oof_non_rounded, thresholds):\n",
        "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
        "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
        "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
        "\n",
        "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
        "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
        "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
        "\n",
        "def TrainML(model_class, test_data):\n",
        "    X = train.drop(['sii'], axis=1)\n",
        "    y = train['sii']\n",
        "\n",
        "\n",
        "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "\n",
        "    train_S = []\n",
        "    test_S = []\n",
        "\n",
        "    oof_non_rounded = np.zeros(len(y), dtype=float)\n",
        "    oof_rounded = np.zeros(len(y), dtype=int)\n",
        "    test_preds = np.zeros((len(test_data), n_splits))\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_yal = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "        model = clone(model_class)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_val_pred = model.predict(X_val)\n",
        "\n",
        "        oof_non_rounded[test_idx] = y_val_pred\n",
        "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
        "        oof_rounded[test_idx] = y_val_pred_rounded\n",
        "\n",
        "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
        "        val_kappa = quadratic_weighted_kappa(y_yal, y_val_pred_rounded)\n",
        "\n",
        "        train_S.append(train_kappa)\n",
        "        test_S.append(val_kappa)\n",
        "\n",
        "        test_preds[:, fold] = model.predict(test_data)\n",
        "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
        "        clear_output(wait=True)\n",
        "\n",
        "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
        "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
        "\n",
        "    KappaOPtimizer = minimize(evaluate_predictions,\n",
        "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded),\n",
        "                              method='Nelder-Mead')\n",
        "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
        "\n",
        "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
        "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
        "\n",
        "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
        "\n",
        "    tpm = test_preds.mean(axis=1)\n",
        "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        'id': sample['id'],\n",
        "        'sii': tpTuned\n",
        "    })\n",
        "\n",
        "    return submission"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-30T13:30:50.709433Z",
          "iopub.execute_input": "2024-10-30T13:30:50.709869Z",
          "iopub.status.idle": "2024-10-30T13:30:50.74295Z",
          "shell.execute_reply.started": "2024-10-30T13:30:50.70981Z",
          "shell.execute_reply": "2024-10-30T13:30:50.741562Z"
        },
        "trusted": true,
        "id": "X5jXkPsQBSlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_origin = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
        "test_origin = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
        "sample_origin = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
        "\n",
        "\n",
        "train_ts_origin = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
        "test_ts_origin = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-30T13:30:50.746863Z",
          "iopub.execute_input": "2024-10-30T13:30:50.748045Z",
          "iopub.status.idle": "2024-10-30T13:32:41.191928Z",
          "shell.execute_reply.started": "2024-10-30T13:30:50.747977Z",
          "shell.execute_reply": "2024-10-30T13:32:41.190283Z"
        },
        "trusted": true,
        "id": "iW4AQt17BSlH",
        "outputId": "af50b6f0-aee3-4317-d503-ac9e4b393964"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|██████████| 996/996 [01:49<00:00,  9.06it/s]\n100%|██████████| 2/2 [00:00<00:00,  8.46it/s]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New: TabNet\n",
        "\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_tabnet.callbacks import Callback\n",
        "import os\n",
        "import torch\n",
        "from pytorch_tabnet.callbacks import Callback\n",
        "\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "import torch\n",
        "\n",
        "class TabNetWrapper(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.model = TabNetRegressor(**kwargs)\n",
        "        self.kwargs = kwargs\n",
        "        self.imputer = SimpleImputer(strategy='median')\n",
        "        self.best_model_path = 'best_tabnet_model.pt'\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Handle missing values\n",
        "        X_imputed = self.imputer.fit_transform(X)\n",
        "\n",
        "        if hasattr(y, 'values'):\n",
        "            y = y.values\n",
        "\n",
        "        # Create internal validation set\n",
        "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "            X_imputed,\n",
        "            y,\n",
        "            test_size=0.2,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Train TabNet model\n",
        "        history = self.model.fit(\n",
        "            X_train=X_train,\n",
        "            y_train=y_train.reshape(-1, 1),\n",
        "            eval_set=[(X_valid, y_valid.reshape(-1, 1))],\n",
        "            eval_name=['valid'],\n",
        "            eval_metric=['mse'],\n",
        "            max_epochs=500,\n",
        "            patience=50,\n",
        "            batch_size=1024,\n",
        "            virtual_batch_size=128,\n",
        "            num_workers=0,\n",
        "            drop_last=False,\n",
        "            callbacks=[\n",
        "                TabNetPretrainedModelCheckpoint(\n",
        "                    filepath=self.best_model_path,\n",
        "                    monitor='valid_mse',\n",
        "                    mode='min',\n",
        "                    save_best_only=True,\n",
        "                    verbose=True\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Load the best model\n",
        "        if os.path.exists(self.best_model_path):\n",
        "            self.model.load_model(self.best_model_path)\n",
        "            os.remove(self.best_model_path)  # Remove temporary file\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_imputed = self.imputer.transform(X)\n",
        "        return self.model.predict(X_imputed).flatten()\n",
        "\n",
        "    def __deepcopy__(self, memo):\n",
        "        # Add deepcopy support for scikit-learn\n",
        "        cls = self.__class__\n",
        "        result = cls.__new__(cls)\n",
        "        memo[id(self)] = result\n",
        "        for k, v in self.__dict__.items():\n",
        "            setattr(result, k, deepcopy(v, memo))\n",
        "        return result\n",
        "\n",
        "# TabNet hyperparameters\n",
        "TabNet_Params = {\n",
        "    'n_d': 64,              # Width of the decision prediction layer\n",
        "    'n_a': 64,              # Width of the attention embedding for each step\n",
        "    'n_steps': 5,           # Number of steps in the architecture\n",
        "    'gamma': 1.5,           # Coefficient for feature selection regularization\n",
        "    'n_independent': 2,     # Number of independent GLU layer in each GLU block\n",
        "    'n_shared': 2,          # Number of shared GLU layer in each GLU block\n",
        "    'lambda_sparse': 1e-4,  # Sparsity regularization\n",
        "    'optimizer_fn': torch.optim.Adam,\n",
        "    'optimizer_params': dict(lr=2e-2, weight_decay=1e-5),\n",
        "    'mask_type': 'entmax',\n",
        "    'scheduler_params': dict(mode=\"min\", patience=10, min_lr=1e-5, factor=0.5),\n",
        "    'scheduler_fn': torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
        "    'verbose': 1,\n",
        "    'device_name': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "}\n",
        "\n",
        "class TabNetPretrainedModelCheckpoint(Callback):\n",
        "    def __init__(self, filepath, monitor='val_loss', mode='min',\n",
        "                 save_best_only=True, verbose=1):\n",
        "        super().__init__()  # Initialize parent class\n",
        "        self.filepath = filepath\n",
        "        self.monitor = monitor\n",
        "        self.mode = mode\n",
        "        self.save_best_only = save_best_only\n",
        "        self.verbose = verbose\n",
        "        self.best = float('inf') if mode == 'min' else -float('inf')\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.model = self.trainer  # Use trainer itself as model\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        # Check if current metric is better than best\n",
        "        if (self.mode == 'min' and current < self.best) or \\\n",
        "           (self.mode == 'max' and current > self.best):\n",
        "            if self.verbose:\n",
        "                print(f'\\nEpoch {epoch}: {self.monitor} improved from {self.best:.4f} to {current:.4f}')\n",
        "            self.best = current\n",
        "            if self.save_best_only:\n",
        "                self.model.save_model(self.filepath)  # Save the entire model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-30T13:32:41.194436Z",
          "iopub.execute_input": "2024-10-30T13:32:41.19497Z",
          "iopub.status.idle": "2024-10-30T13:32:41.244913Z",
          "shell.execute_reply.started": "2024-10-30T13:32:41.194911Z",
          "shell.execute_reply": "2024-10-30T13:32:41.243431Z"
        },
        "trusted": true,
        "id": "EHlD-KxDBSlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission"
      ],
      "metadata": {
        "id": "k_17SeACBSlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = train_origin.copy()\n",
        "test = test_origin.copy()\n",
        "sample = sample_origin.copy()\n",
        "train_ts = train_ts_origin.copy()\n",
        "test_ts = test_ts_origin.copy()\n",
        "\n",
        "\n",
        "df_train = train_ts.drop('id', axis=1)\n",
        "df_test = test_ts.drop('id', axis=1)\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, encoding_dim):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, encoding_dim*3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(encoding_dim*3, encoding_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(encoding_dim*2, encoding_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, input_dim*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_dim*4, input_dim*3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_dim*3, input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "\n",
        "def perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32, seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    df_scaled = scaler.fit_transform(df)\n",
        "\n",
        "    data_tensor = torch.FloatTensor(df_scaled)\n",
        "\n",
        "    input_dim = data_tensor.shape[1]\n",
        "    autoencoder = AutoEncoder(input_dim, encoding_dim)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(autoencoder.parameters())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, len(data_tensor), batch_size):\n",
        "            batch = data_tensor[i : i + batch_size]\n",
        "            optimizer.zero_grad()\n",
        "            reconstructed = autoencoder(batch)\n",
        "            loss = criterion(reconstructed, batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoded_data = autoencoder.encoder(data_tensor).numpy()\n",
        "\n",
        "    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n",
        "\n",
        "    return df_encoded\n",
        "\n",
        "train_ts_encoded = perform_autoencoder(df_train, encoding_dim=64, epochs=200, batch_size=64)\n",
        "test_ts_encoded = perform_autoencoder(df_test, encoding_dim=64, epochs=200, batch_size=64)\n",
        "\n",
        "time_series_cols = train_ts_encoded.columns.tolist()\n",
        "train_ts_encoded[\"id\"]=train_ts[\"id\"]\n",
        "test_ts_encoded['id']=test_ts[\"id\"]\n",
        "train = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\n",
        "test = pd.merge(test, test_ts_encoded, how=\"left\", on='id')\n",
        "\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "numeric_cols = train.select_dtypes(include=['int32', 'int64', 'float64', 'int64']).columns\n",
        "imputed_data = imputer.fit_transform(train[numeric_cols])\n",
        "train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n",
        "train_imputed['sii'] = train_imputed['sii'].round().astype(int)\n",
        "for col in train.columns:\n",
        "    if col not in numeric_cols:\n",
        "        train_imputed[col] = train[col]\n",
        "\n",
        "train = train_imputed\n",
        "\n",
        "\n",
        "train = feature_engineering(train)\n",
        "train = train.dropna(thresh=10,axis=0) ## thresh=10\n",
        "test = feature_engineering(test)\n",
        "\n",
        "featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
        "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
        "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
        "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
        "                'Fitness_Endurance-Max_Stage',\n",
        "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
        "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
        "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
        "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
        "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
        "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
        "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
        "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
        "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
        "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
        "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
        "                'SDS-SDS_Total_T',\n",
        "                'PreInt_EduHx-computerinternet_hoursday', 'sii', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n",
        "                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n",
        "                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW']\n",
        "\n",
        "featuresCols += time_series_cols\n",
        "\n",
        "train = train[featuresCols]\n",
        "train = train.dropna(subset='sii')\n",
        "\n",
        "featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
        "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
        "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
        "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
        "                'Fitness_Endurance-Max_Stage',\n",
        "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
        "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
        "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
        "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
        "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
        "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
        "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
        "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
        "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
        "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
        "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
        "                'SDS-SDS_Total_T',\n",
        "                'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n",
        "                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n",
        "                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW']\n",
        "featuresCols += time_series_cols\n",
        "\n",
        "test = test[featuresCols]\n",
        "\n",
        "if np.any(np.isinf(train)):\n",
        "    train = train.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "TabNet_Model = TabNetWrapper(**TabNet_Params) # New\n",
        "\n",
        "# ensemble = VotingRegressor(estimators=[\n",
        "#     ('lgb', Pipeline(steps=[('imputer', imputer),('regressor', LGBMRegressor(random_state=SEED))])),\n",
        "#     ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n",
        "#     ('cat', Pipeline(steps=[('imputer', imputer),  ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n",
        "#     ('rf', Pipeline(steps=[('imputer', imputer),  ('regressor', RandomForestRegressor(random_state=SEED))])),\n",
        "#     ('gb', Pipeline(steps=[('imputer', imputer),  ('regressor', GradientBoostingRegressor(random_state=SEED))])),\n",
        "#     ('tab', Pipeline(steps=[('imputer', imputer), ('regressor',TabNet_Model)]))\n",
        "# ])\n",
        "\n",
        "# Model parameters for LightGBM\n",
        "Params = {\n",
        "    'learning_rate': 0.046,\n",
        "    'max_depth': 12,\n",
        "    'num_leaves': 478,\n",
        "    'min_data_in_leaf': 13,\n",
        "    'feature_fraction': 0.893,\n",
        "    'bagging_fraction': 0.784,\n",
        "    'bagging_freq': 4,\n",
        "    'lambda_l1': 10,  # Increased from 6.59\n",
        "    'lambda_l2': 0.01  # Increased from 2.68e-06\n",
        "}\n",
        "\n",
        "\n",
        "XGB_Params = {\n",
        "    'learning_rate': 0.05,\n",
        "    'max_depth': 6,\n",
        "    'n_estimators': 200,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'reg_alpha': 1,  # Increased from 0.1\n",
        "    'reg_lambda': 5,  # Increased from 1\n",
        "    'random_state': SEED,\n",
        "    'tree_method': 'exact'\n",
        "}\n",
        "\n",
        "\n",
        "CatBoost_Params = {\n",
        "    'learning_rate': 0.05,\n",
        "    'depth': 6,\n",
        "    'iterations': 200,\n",
        "    'random_seed': SEED,\n",
        "    'verbose': 0,\n",
        "    'l2_leaf_reg': 10  # Increase this value\n",
        "}\n",
        "# Create model instances\n",
        "Light = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\n",
        "XGB_Model = XGBRegressor(**XGB_Params)\n",
        "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
        "\n",
        "\n",
        "# Combine models using Voting Regressor\n",
        "voting_model = VotingRegressor(estimators=[\n",
        "    ('lightgbm',  Light),\n",
        "    ('xgboost',  XGB_Model),\n",
        "    ('catboost', CatBoost_Model),\n",
        "    ('tabnet', TabNet_Model)  # New:TabNet\n",
        "])\n",
        "\n",
        "\n",
        "Submission1 = TrainML(voting_model, test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-30T13:52:46.589137Z",
          "iopub.execute_input": "2024-10-30T13:52:46.589583Z",
          "iopub.status.idle": "2024-10-30T13:58:47.496191Z",
          "shell.execute_reply.started": "2024-10-30T13:52:46.589539Z",
          "shell.execute_reply": "2024-10-30T13:58:47.493823Z"
        },
        "trusted": true,
        "id": "79KLYA8kBSlJ",
        "outputId": "fbc26f76-6597-41d8-e714-8d11b934d9ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Training Folds: 100%|██████████| 5/5 [05:27<00:00, 65.45s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Mean Train QWK --> 0.7457\nMean Validation QWK ---> 0.4800\n----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.534\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Submission1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-30T14:00:08.020814Z",
          "iopub.execute_input": "2024-10-30T14:00:08.021364Z",
          "iopub.status.idle": "2024-10-30T14:00:08.038294Z",
          "shell.execute_reply.started": "2024-10-30T14:00:08.021317Z",
          "shell.execute_reply": "2024-10-30T14:00:08.036904Z"
        },
        "trusted": true,
        "id": "Du4xY0fxBSlJ",
        "outputId": "9d0a563e-61d0-4823-e5bc-01a95a5df08e"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 24,
          "output_type": "execute_result",
          "data": {
            "text/plain": "          id  sii\n0   00008ff9    0\n1   000fd460    0\n2   00105258    1\n3   00115b9f    2\n4   0016bb22    0\n5   001f3379    0\n6   0038ba98    1\n7   0068a485    0\n8   0069fbed    1\n9   0083e397    0\n10  0087dd65    0\n11  00abe655    0\n12  00ae59c9    1\n13  00af6387    1\n14  00bd4359    1\n15  00c0cd71    1\n16  00d56d4b    0\n17  00d9913d    0\n18  00e6167c    1\n19  00ebc35d    1",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sii</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>001f3379</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0038ba98</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0068a485</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0069fbed</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0083e397</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0087dd65</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>00abe655</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>00ae59c9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>00af6387</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>00bd4359</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>00c0cd71</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>00d56d4b</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>00d9913d</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>00e6167c</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>00ebc35d</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.538 0.529"
      ],
      "metadata": {
        "id": "2bv2lpdtBSlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = train_origin.copy()\n",
        "test = test_origin.copy()\n",
        "sample = sample_origin.copy()\n",
        "train_ts = train_ts_origin.copy()\n",
        "test_ts = test_ts_origin.copy()\n",
        "\n",
        "time_series_cols = train_ts.columns.tolist()\n",
        "time_series_cols.remove(\"id\")\n",
        "\n",
        "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
        "test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
        "\n",
        "train = train.drop('id', axis=1)\n",
        "test = test.drop('id', axis=1)\n",
        "\n",
        "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
        "                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
        "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
        "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
        "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
        "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
        "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
        "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
        "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
        "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
        "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
        "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
        "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
        "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
        "                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
        "                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
        "                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
        "                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n",
        "\n",
        "featuresCols += time_series_cols\n",
        "\n",
        "train = train[featuresCols]\n",
        "train = train.dropna(subset='sii')\n",
        "\n",
        "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season',\n",
        "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season',\n",
        "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
        "\n",
        "def update(df):\n",
        "    global cat_c\n",
        "    for c in cat_c:\n",
        "        df[c] = df[c].fillna('Missing')\n",
        "        df[c] = df[c].astype('category')\n",
        "    return df\n",
        "\n",
        "train = update(train)\n",
        "test = update(test)\n",
        "\n",
        "def create_mapping(column, dataset):\n",
        "    unique_values = dataset[column].unique()\n",
        "    return {value: idx for idx, value in enumerate(unique_values)}\n",
        "\n",
        "for col in cat_c:\n",
        "    mapping = create_mapping(col, train)\n",
        "    mappingTe = create_mapping(col, test)\n",
        "\n",
        "    train[col] = train[col].replace(mapping).astype(int)\n",
        "    test[col] = test[col].replace(mappingTe).astype(int)\n",
        "\n",
        "# train = feature_engineering(train,False)\n",
        "train = train.dropna(thresh=10,axis=0) ## thresh=10\n",
        "# test = feature_engineering(test,False)\n",
        "\n",
        "# Model parameters for LightGBM\n",
        "Params = {\n",
        "    'learning_rate': 0.046,\n",
        "    'max_depth': 12,\n",
        "    'num_leaves': 478,\n",
        "    'min_data_in_leaf': 13,\n",
        "    'feature_fraction': 0.893,\n",
        "    'bagging_fraction': 0.784,\n",
        "    'bagging_freq': 4,\n",
        "    'lambda_l1': 10,  # Increased from 6.59\n",
        "    'lambda_l2': 0.01  # Increased from 2.68e-06\n",
        "}\n",
        "\n",
        "\n",
        "# XGBoost parameters\n",
        "XGB_Params = {\n",
        "    'learning_rate': 0.05,\n",
        "    'max_depth': 6,\n",
        "    'n_estimators': 200,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'reg_alpha': 1,  # Increased from 0.1\n",
        "    'reg_lambda': 5,  # Increased from 1\n",
        "    'random_state': SEED\n",
        "}\n",
        "\n",
        "\n",
        "CatBoost_Params = {\n",
        "    'learning_rate': 0.05,\n",
        "    'depth': 6,\n",
        "    'iterations': 200,\n",
        "    'random_seed': SEED,\n",
        "    'cat_features': cat_c,\n",
        "    'verbose': 0,\n",
        "    'l2_leaf_reg': 10  # Increase this value\n",
        "}\n",
        "\n",
        "# Create model instances\n",
        "Light = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\n",
        "XGB_Model = XGBRegressor(**XGB_Params)\n",
        "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
        "\n",
        "\n",
        "# Combine models using Voting Regressor\n",
        "voting_model = VotingRegressor(estimators=[\n",
        "    ('lightgbm',  Light),\n",
        "    ('xgboost',  XGB_Model),\n",
        "    ('catboost', CatBoost_Model),\n",
        "#     ('tabnet', TabNet_Model)  # New:TabNet\n",
        "])\n",
        "\n",
        "\n",
        "# Train the ensemble model\n",
        "Submission2 = TrainML(voting_model, test)\n",
        "\n",
        "# Save submission\n",
        "#Submission2.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-30T14:06:38.642755Z",
          "iopub.execute_input": "2024-10-30T14:06:38.643347Z",
          "iopub.status.idle": "2024-10-30T14:07:48.400811Z",
          "shell.execute_reply.started": "2024-10-30T14:06:38.643299Z",
          "shell.execute_reply": "2024-10-30T14:07:48.399295Z"
        },
        "trusted": true,
        "id": "LTWiYRZpBSlJ",
        "outputId": "705b5910-9f47-4d80-defd-5d8e149663b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Training Folds: 100%|██████████| 5/5 [01:09<00:00, 13.81s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Mean Train QWK --> 0.7595\nMean Validation QWK ---> 0.3926\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.457\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "execution_count": 26,
          "output_type": "execute_result",
          "data": {
            "text/plain": "          id  sii\n0   00008ff9    1\n1   000fd460    0\n2   00105258    0\n3   00115b9f    0\n4   0016bb22    0\n5   001f3379    1\n6   0038ba98    0\n7   0068a485    0\n8   0069fbed    1\n9   0083e397    0\n10  0087dd65    0\n11  00abe655    0\n12  00ae59c9    1\n13  00af6387    1\n14  00bd4359    1\n15  00c0cd71    1\n16  00d56d4b    0\n17  00d9913d    0\n18  00e6167c    0\n19  00ebc35d    1",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sii</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>001f3379</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0038ba98</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0068a485</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0069fbed</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0083e397</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0087dd65</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>00abe655</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>00ae59c9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>00af6387</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>00bd4359</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>00c0cd71</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>00d56d4b</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>00d9913d</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>00e6167c</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>00ebc35d</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.448(with fearture engineering) 0.458(without fearture engineering) 0.449"
      ],
      "metadata": {
        "id": "64KS95GaBSlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
        "                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
        "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
        "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
        "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
        "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
        "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
        "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
        "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
        "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
        "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
        "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
        "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
        "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
        "                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
        "                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
        "                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
        "                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n",
        "\n",
        "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season',\n",
        "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season',\n",
        "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
        "\n",
        "train = train_origin.copy()\n",
        "test = test_origin.copy()\n",
        "sample = sample_origin.copy()\n",
        "train_ts = train_ts_origin.copy()\n",
        "test_ts = test_ts_origin.copy()\n",
        "\n",
        "time_series_cols = train_ts.columns.tolist()\n",
        "time_series_cols.remove(\"id\")\n",
        "\n",
        "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
        "test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
        "\n",
        "train = train.drop('id', axis=1)\n",
        "test = test.drop('id', axis=1)\n",
        "\n",
        "featuresCols += time_series_cols\n",
        "\n",
        "train = train[featuresCols]\n",
        "train = train.dropna(subset='sii')\n",
        "\n",
        "def update(df):\n",
        "    global cat_c\n",
        "    for c in cat_c:\n",
        "        df[c] = df[c].fillna('Missing')\n",
        "        df[c] = df[c].astype('category')\n",
        "    return df\n",
        "\n",
        "train = update(train)\n",
        "test = update(test)\n",
        "\n",
        "def create_mapping(column, dataset):\n",
        "    unique_values = dataset[column].unique()\n",
        "    return {value: idx for idx, value in enumerate(unique_values)}\n",
        "\n",
        "for col in cat_c:\n",
        "    mapping = create_mapping(col, train)\n",
        "    mappingTe = create_mapping(col, test)\n",
        "\n",
        "    train[col] = train[col].replace(mapping).astype(int)\n",
        "    test[col] = test[col].replace(mappingTe).astype(int)\n",
        "\n",
        "# train = feature_engineering(train)\n",
        "# train = train.dropna(thresh=10,axis=0) ## thresh=10\n",
        "# test = feature_engineering(test)\n",
        "\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "ensemble = VotingRegressor(estimators=[\n",
        "    ('lgb', Pipeline(steps=[('imputer', imputer),  ('regressor', LGBMRegressor(random_state=SEED))])),\n",
        "    ('xgb', Pipeline(steps=[('imputer', imputer),  ('regressor', XGBRegressor(random_state=SEED))])),\n",
        "    ('cat', Pipeline(steps=[('imputer', imputer),  ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n",
        "    ('rf', Pipeline(steps=[('imputer', imputer),  ('regressor', RandomForestRegressor(random_state=SEED))])),\n",
        "    ('gb', Pipeline(steps=[('imputer', imputer),  ('regressor', GradientBoostingRegressor(random_state=SEED))])),\n",
        "    ('tab', Pipeline(steps=[('imputer', imputer), ('regressor',TabNet_Model)]))\n",
        "])\n",
        "\n",
        "Submission3 = TrainML(ensemble, test)\n",
        "# Submission3 = pd.DataFrame({\n",
        "#     'id': sample['id'],\n",
        "#     'sii': Submission3\n",
        "# })"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-30T13:43:14.712834Z",
          "iopub.execute_input": "2024-10-30T13:43:14.713411Z",
          "iopub.status.idle": "2024-10-30T13:48:01.047546Z",
          "shell.execute_reply.started": "2024-10-30T13:43:14.713351Z",
          "shell.execute_reply": "2024-10-30T13:48:01.046242Z"
        },
        "trusted": true,
        "id": "VdGg58oFBSlK",
        "outputId": "5e028e13-4fed-4ae8-92fa-8c4e16ff79b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Training Folds: 100%|██████████| 5/5 [04:45<00:00, 57.15s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Mean Train QWK --> 0.8493\nMean Validation QWK ---> 0.3633\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.441\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "execution_count": 18,
          "output_type": "execute_result",
          "data": {
            "text/plain": "          id  sii\n0   00008ff9    1\n1   000fd460    0\n2   00105258    0\n3   00115b9f    0\n4   0016bb22    1\n5   001f3379    1\n6   0038ba98    0\n7   0068a485    0\n8   0069fbed    1\n9   0083e397    0\n10  0087dd65    1\n11  00abe655    0\n12  00ae59c9    1\n13  00af6387    1\n14  00bd4359    1\n15  00c0cd71    2\n16  00d56d4b    0\n17  00d9913d    0\n18  00e6167c    0\n19  00ebc35d    1",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sii</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>001f3379</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0038ba98</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0068a485</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0069fbed</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0083e397</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0087dd65</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>00abe655</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>00ae59c9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>00af6387</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>00bd4359</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>00c0cd71</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>00d56d4b</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>00d9913d</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>00e6167c</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>00ebc35d</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3"
      ],
      "metadata": {
        "id": "N6RtFAO1BSlK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.469"
      ],
      "metadata": {
        "id": "JGqmG0-vBSlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Submission3"
      ],
      "metadata": {
        "id": "KATPA7MQBSlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge Submission"
      ],
      "metadata": {
        "id": "PH0nr3I8BSlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 假设你有多个 Submission 数据框\n",
        "submissions = [Submission1, Submission2, Submission3]\n",
        "submission_names = ['sii_1', 'sii_2', 'sii_3']\n",
        "\n",
        "# 对每个 Submission 进行排序和重置索引\n",
        "for i, sub in enumerate(submissions):\n",
        "    submissions[i] = sub.sort_values(by='id').reset_index(drop=True)\n",
        "\n",
        "# 创建一个新的 DataFrame 用于合并结果\n",
        "combined = pd.DataFrame({'id': submissions[0]['id']})\n",
        "\n",
        "# 将每个 Submission 的结果添加到 combined DataFrame\n",
        "for name, sub in zip(submission_names, submissions):\n",
        "    combined[name] = sub['sii']\n",
        "\n",
        "def majority_vote(row):\n",
        "    return row.mode()[0]\n",
        "\n",
        "# 进行投票\n",
        "combined['final_sii'] = combined[submission_names].apply(majority_vote, axis=1)\n",
        "\n",
        "# 准备最终的提交结果\n",
        "final_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n",
        "\n",
        "# 保存到 CSV 文件\n",
        "final_submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Majority voting completed and saved to 'submission.csv'\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-30T13:48:01.049566Z",
          "iopub.execute_input": "2024-10-30T13:48:01.050116Z",
          "iopub.status.idle": "2024-10-30T13:48:01.083941Z",
          "shell.execute_reply.started": "2024-10-30T13:48:01.050037Z",
          "shell.execute_reply": "2024-10-30T13:48:01.082428Z"
        },
        "trusted": true,
        "id": "rRG-_2cOBSlK",
        "outputId": "cb5c249a-fce5-4bf9-85a4-50db9aabdee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Majority voting completed and saved to 'submission.csv'\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_submission"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-30T13:48:01.085699Z",
          "iopub.execute_input": "2024-10-30T13:48:01.086231Z",
          "iopub.status.idle": "2024-10-30T13:48:01.104439Z",
          "shell.execute_reply.started": "2024-10-30T13:48:01.086165Z",
          "shell.execute_reply": "2024-10-30T13:48:01.102793Z"
        },
        "trusted": true,
        "id": "xSCU13wvBSlK",
        "outputId": "8a2fd4fc-bc32-4c0b-e360-e204d89f909a"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 20,
          "output_type": "execute_result",
          "data": {
            "text/plain": "          id  sii\n0   00008ff9    1\n1   000fd460    0\n2   00105258    0\n3   00115b9f    0\n4   0016bb22    1\n5   001f3379    1\n6   0038ba98    0\n7   0068a485    0\n8   0069fbed    1\n9   0083e397    0\n10  0087dd65    0\n11  00abe655    0\n12  00ae59c9    1\n13  00af6387    1\n14  00bd4359    1\n15  00c0cd71    1\n16  00d56d4b    0\n17  00d9913d    0\n18  00e6167c    0\n19  00ebc35d    1",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sii</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>001f3379</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0038ba98</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0068a485</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0069fbed</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0083e397</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0087dd65</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>00abe655</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>00ae59c9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>00af6387</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>00bd4359</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>00c0cd71</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>00d56d4b</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>00d9913d</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>00e6167c</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>00ebc35d</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    }
  ]
}